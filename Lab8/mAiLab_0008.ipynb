{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(img_train, lbl_train), (img_test, lbl_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build a Layer with [Size x Size] x Numbers neurons\n",
    "class Layer(object):\n",
    "    def __init__(self, lay_size = []):\n",
    "        self.lay_size = lay_size\n",
    "        self.maps = []\n",
    "        for map_size in lay_size :\n",
    "            self.maps.append(np.zeros(map_size))\n",
    "        self.maps = np.array(self.maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ConvLayer(Layer) :\n",
    "        def __init__(self, lay_size = [], conv_core_sizes = [], map_comb_index = []) :\n",
    "            Layer.__init__(self, lay_size)\n",
    "            self.conv_cores = []\n",
    "            self.bias = []\n",
    "            self.map_comb_index = map_comb_index\n",
    "\n",
    "            # Initialize the parameters. \n",
    "            # The number -2.4/Fi and 2.4/Fi comes from the paper in Appendices A.\n",
    "            for conv_core_size in conv_core_sizes :\n",
    "                # Fi is based on the definition in the paper\n",
    "                Fi = conv_core_size[0] * conv_core_size[1] + 1\n",
    "                # Make random filters\n",
    "                self.conv_cores.append(np.random.uniform(-2.4/Fi, 2.4/Fi, conv_core_size)) \n",
    "                # Make random biases\n",
    "                self.bias.append(np.random.uniform(-2.4/Fi, 2.4/Fi))\n",
    "\n",
    "            self.conv_cores = np.array(self.conv_cores)\n",
    "        \n",
    "        def cov_op(self, pre_maps, core_index) :\n",
    "            pre_map_shape = pre_maps.shape\n",
    "            core_shape = self.conv_cores[core_index].shape\n",
    "            map_shape = self.maps[core_index].shape\n",
    "\n",
    "            # Check the input size, \n",
    "            # If the result from input size match the output\n",
    "            # Calculate the Convolution Layer\n",
    "            if not (map_shape[-2] == pre_map_shape[-2] - core_shape[-2] + 1 \\\n",
    "                and map_shape[-1] == pre_map_shape[-1] - core_shape[-1] + 1) :\n",
    "                return None\n",
    "\n",
    "            for i in range(map_shape[-2]) :\n",
    "                for j in range(map_shape[-1]) :\n",
    "\n",
    "                    # Filter caculation in HW4 \n",
    "                    local_recept = pre_maps[:, i:i + core_shape[-2], j:j + core_shape[-1]]\n",
    "                    val = np.sum(local_recept * self.conv_cores[core_index]) + self.bias[core_index]\n",
    "\n",
    "                    # Use tanh(x) as activation function.\n",
    "                    # Remark that tanh(x) = ((e^2x) -1)/((e^2x)+1)\n",
    "                    # We use the parameters in the paper\n",
    "                    # f(a) = Atanh(Sa), where A = 1.7159, S = 2/3\n",
    "\n",
    "                    val = np.exp((4.0/3)*val) \n",
    "                    self.maps[core_index][i][j] = 1.7159 * (val - 1) / (val + 1)\n",
    "        \n",
    "        def calc_maps(self, pre_mapset, map_comb_flag = False) :\n",
    "            # if mapcombflag = False, the first Conv Layer\n",
    "            if not map_comb_flag :\n",
    "                for i in range(len(self.maps)) :\n",
    "                    self.cov_op(pre_mapset, i)\n",
    "                \n",
    "            # mapcombflag = True, the other Conv Layer\n",
    "            else :\n",
    "                for i in range(len(self.maps)) :\n",
    "                    self.cov_op(pre_mapset[self.map_comb_index[i]], i)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class PoolingLayer(Layer) :\n",
    "    def __init__(self, lay_size = [], pool_core_sizes = []) :\n",
    "        Layer.__init__(self, lay_size)\n",
    "        Fi = pool_core_sizes[0][0] * pool_core_sizes[0][1] + 1\n",
    "        self.poolparas = np.random.uniform(-2.4/Fi, 2.4/Fi, [len(lay_size), 2])\n",
    "        self.poolcore_sizes = np.array(pool_core_sizes)\n",
    "        \n",
    "    def pool_op(self, pre_map, pool_index) :\n",
    "        pre_map_shape = pre_map.shape\n",
    "        poolcore_size = self.poolcore_sizes[pool_index]\n",
    "        for i in range(int(pre_map_shape[0] / poolcore_size[0])) :\n",
    "            for j in range(int(pre_map_shape[1] / poolcore_size[1])) :\n",
    "                val = self.poolparas[pool_index][0] * np.sum(pre_map[i*poolcore_size[0]:(i+1)*poolcore_size[0],\\\n",
    "                            j*poolcore_size[1]:(j+1)*poolcore_size[1]]) + self.poolparas[pool_index][1]\n",
    "                val = np.exp((4.0/3)*val)\n",
    "                self.maps[pool_index][i][j] = 1.7159 * (val -1) / (val + 1)\n",
    "\n",
    "    def calc_maps(self, pre_mapset) :\n",
    "        for i in range(len(self.maps)) :\n",
    "            self.pool_op(pre_mapset[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class FcLayer(Layer) :\n",
    "    def __init__(self, lay_len, pre_nodesnum) :\n",
    "        Layer.__init__(self, [[1, lay_len]])\n",
    "        Fi = pre_nodesnum + 1\n",
    "        self.weight = np.random.uniform(-2.4/Fi, 2.4/Fi, [lay_len, pre_nodesnum]) # 84x120\n",
    "        self.bias = np.random.uniform(-2.4/Fi, 2.4/Fi, [lay_len]) #84 \n",
    "        \n",
    "    def fc_op(self, pre_maps, node_index) :\n",
    "        # pre_maps: 1x1x120\n",
    "        pre_nodes = pre_maps.reshape([pre_maps.shape[0] * pre_maps.shape[1] * pre_maps.shape[2]]) #120\n",
    "        val  = np.sum(self.weight[node_index] * pre_nodes) + self.bias[node_index] #\n",
    "        val = np.exp((4.0/3)*val)\n",
    "        self.maps[0][0][node_index] = 1.7159 *  (val -1) / (val + 1)\n",
    "\n",
    "    def calc_maps(self, pre_mapset) :\n",
    "        for i in range(len(self.maps[0][0])) : #84\n",
    "            self.fc_op(pre_mapset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class OutputLayer(FcLayer) :\n",
    "    def __init__(self, lay_len, pre_nodesnum) :\n",
    "        FcLayer.__init__(self, lay_len, pre_nodesnum)\n",
    "        # We have to assign float64 to ensure the weight is float number. \n",
    "        self.weight = np.float64(np.random.choice([-1,1], [lay_len, pre_nodesnum])) # 10x84\n",
    "\n",
    "    def rbf(self, pre_maps, node_index = -1) :\n",
    "        pre_nodes = pre_maps.flatten() #84\n",
    "\n",
    "        if node_index != -1:\n",
    "            self.maps[0][0][node_index] = 0.5 * np.sum((pre_nodes - self.weight[node_index])**2)\n",
    "\n",
    "        else:\n",
    "            for i in range(len(self.maps[0][0])) :\n",
    "                self.maps[0][0][i] = 0.5 * np.sum((pre_nodes - self.weight[i])**2)\n",
    "    \n",
    "    def back_propa(self, pre_mapset, current_error, learn_rate) :\n",
    "        self.current_error = current_error\n",
    "        current_error_matrix = np.array(np.matrix(list(current_error[0]) * self.weight.shape[1]).T)  #84x10 > 10x84\n",
    "        \n",
    "        # pre_mapset : 1x1x84\n",
    "        weight_update = (self.weight - np.array(list(pre_mapset[0]) * self.weight.shape[0])) * current_error_matrix   \n",
    "        self.weight -= learn_rate * weight_update\n",
    "        pre_error = ((np.array(list(pre_mapset[0]) * self.weight.shape[0]) - self.weight) * current_error_matrix).sum(axis = 0)\n",
    "        return pre_error.reshape(pre_mapset.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(object): \n",
    "    def __init__(self) :\n",
    "\n",
    "        C3_core_sizes = [[3, 5, 5]] * 6\n",
    "        C3_core_sizes.extend([[4, 5, 5]] * 9)\n",
    "        C3_core_sizes.extend([[6, 5, 5]])\n",
    "        \n",
    "        # C3_map_comb_index is based on TABLE 1\n",
    "        C3_map_comb_index = [[0,1,2],[1,2,3],[2,3,4],[3,4,5],[4,5,0],[5,0,1], \\\n",
    "             [0,1,2,3],[1,2,3,4],[2,3,4,5],[3,4,5,0],[4,5,0,1],[5,0,1,2],[0,1,3,4],[1,2,4,5],[0,2,3,5],[0,1,2,3,4,5]]\n",
    "\n",
    "        self.C1 = ConvLayer([[28, 28]] * 6, [[1, 5, 5]] * 6)\n",
    "        self.S2 = PoolingLayer([[14, 14]] * 6, [[2, 2]] * 6)\n",
    "        self.C3 = ConvLayer([[10, 10]] * 16, C3_core_sizes, C3_map_comb_index)\n",
    "        self.S4 = PoolingLayer([[5, 5]] * 16, [[2, 2]] * 16)\n",
    "        self.C5 = ConvLayer([[1, 1]] * 120, [[16, 5, 5]] * 120)\n",
    "        self.F6 = FcLayer(84, 120)\n",
    "        self.output = OutputLayer(10, 84)\n",
    "        \n",
    "    def fw_prop(self, mapset, mapclass = -1) :\n",
    "        self.C1.calc_maps(mapset)\n",
    "        self.S2.calc_maps(self.C1.maps)\n",
    "        self.C3.calc_maps(self.S2.maps, True)\n",
    "        self.S4.calc_maps(self.C3.maps)\n",
    "        self.C5.calc_maps(self.S4.maps)\n",
    "        self.F6.calc_maps(self.C5.maps)\n",
    "        self.output.rbf(self.F6.maps, mapclass)\n",
    "    \n",
    "    def bw_prop(self, mapset, mapclass, learn_rate) :\n",
    "        output_error = np.zeros([1, 1, 10])\n",
    "        output_error[0][0][mapclass] = 1\n",
    "        \n",
    "        F6_error = self.output.back_propa(self.F6.maps, output_error, learn_rate, True)\n",
    "        C5_error = self.F6.back_propa(self.C5.maps, F6_error, learn_rate, True)\n",
    "        S4_error = self.C5.back_propa(self.S4.maps, C5_error, learn_rate, True)\n",
    "        C3_error = self.S4.back_propa(self.C3.maps, S4_error, learn_rate, True)\n",
    "        S2_error = self.C3.back_propa(self.S2.maps, C3_error, learn_rate, True)\n",
    "        C1_error = self.S2.back_propa(self.C1.maps, S2_error, learn_rate, True)\n",
    "        ilayer_error = self.C1.back_propa(mapset, C1_error, learn_rate, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "processed_imgs = []\n",
    "for index in range(1):\n",
    "    padded_img = np.ones([32, 32]) * -0.1\n",
    "    \n",
    "    for row in range(img_train[index].shape[0]):\n",
    "        for col in range(img_train[index].shape[1]):\n",
    "            if img_train[index][row][col] > 0:\n",
    "                padded_img[row + 2][col + 2] = 1.175\n",
    "    processed_imgs.append(padded_img)\n",
    "processed_imgs = np.array(processed_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_covnet = ConvNet()\n",
    "train_covnet.fw_prop(np.array(processed_imgs), lbl_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mapset = processed_imgs\n",
    "mapclass = lbl_train[0]\n",
    "learn_rate = 0.001\n",
    "output_error = np.zeros([1, 1, 10])\n",
    "output_error[0][0][mapclass] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = train_covnet.output.weight\n",
    "current_error = output_error\n",
    "current_error_matrix = np.tile(current_error[0], (weight.shape[1], 1)).T\n",
    "weight_update = (weight - np.tile(train_covnet.F6.maps[0], (weight.shape[0], 1))) * current_error_matrix \n",
    "\n",
    "weight -= weight_update \n",
    "pre_error = ((np.array(list(train_covnet.F6.maps[0]) * weight.shape[0]) - weight) * current_error_matrix).sum(axis = 0)\n",
    "pre_error.reshape(train_covnet.F6.maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
